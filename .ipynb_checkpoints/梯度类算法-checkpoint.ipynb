{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO问题的梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO问题\n",
    "<p>Lasso是Least Absolute Shrinkage and Selection Operator的简称，是一种采用了L1正则化（L1-regularization)的线性回归方法，采用了L1正则会使得部分学习到的特征权值为0，从而达到稀疏化和特征选择的目的。</p>\n",
    "<b>LASSO & Ridge Regression(岭回归)<b/>\n",
    "<p>* LASSO:  L1 正则</p>\n",
    "<p>* Ridge Regression： L2 正则</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、 LASSO 问题的 Huber 光滑化梯度法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑LASSO问题\n",
    "$$\n",
    "\\displaystyle\\min_x \\frac{1}{2}\\|Ax-b\\|_2^2 + \\mu \\|x\\|_1.\n",
    "$$\n",
    "由于目标函数的不光滑性，在某些点处无法求出梯度，因此不能直接对原始问题使用梯度法求解。\n",
    "<p>这里考虑一维光滑函数：</p>\n",
    "$$\n",
    "\\displaystyle\\ell_\\sigma(x)=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\frac{1}{2\\sigma}x^2, & |x|<\\sigma; \\\\\n",
    "|x|-\\frac{\\sigma}{2}, & \\mathrm{otherwise}.\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "使用 $\\displaystyle L_\\sigma(x)=\\sum_{i=1}^n\\ell_\\sigma(x_i)$ 替代 $\\|x\\|_1$，得到如下优化问题：\n",
    "$$\n",
    "\\displaystyle\\min_x f(x) := \\frac{1}{2}\\|Ax-b\\|_2^2 + \\mu L_{\\sigma}(x).\n",
    "$$\n",
    "在 x点处 f的梯度为：\n",
    "$$\n",
    "\\displaystyle\\nabla f(x)=A^\\top (Ax-b)+\\mu\\nabla L_{\\sigma}(x),\n",
    "$$\n",
    "其中\n",
    "$$\n",
    "\\displaystyle(\\nabla L_{\\sigma}(x))_i=\\left\\{ \\begin{array}{ll}\n",
    "\\mathrm{sign}(x_i), & |x_i|>\\sigma; \\\\\n",
    "\\frac{x_i}{\\sigma}, & |x_i|\\le\\sigma.\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "可以直接利用带BB步长的梯度下降法最小化 f来得到原问题解的一个近似解（解的质量与 σ 选取有关）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nopts.maxit ：最大迭代次数\\nopts.ftol ：针对函数值的停机准则，当相邻两次迭代函数值之差小于该值时认为该条件满足\\nopts.gtol ：针对梯度的停机准则，当当前步梯度范数小于该值时认为该条件满足\\nopts.alpha0 ：步长的初始值\\nopts.sigma ：Huber 光滑化参数 σ\\nopts.verbose ：不为 0 时输出每步迭代信息，否则不输出\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "opts = {'maxit': 200, 'ftol': 1e-8, 'gtol': 1e-6, 'alpha0': 0.01, \n",
    "           'sigma': 0.1, 'verbose': 0}\n",
    "\n",
    "'''\n",
    "opts.maxit ：最大迭代次数\n",
    "opts.ftol ：针对函数值的停机准则，当相邻两次迭代函数值之差小于该值时认为该条件满足\n",
    "opts.gtol ：针对梯度的停机准则，当当前步梯度范数小于该值时认为该条件满足\n",
    "opts.alpha0 ：步长的初始值\n",
    "opts.sigma ：Huber 光滑化参数 σ\n",
    "opts.verbose ：不为 0 时输出每步迭代信息，否则不输出\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LASSO_grad_huber_inn(x, A, b, mu, mu0, opt):\n",
    "    for i in opts.keys():\n",
    "        if opt.get(i, -1) == -1:\n",
    "            opt[i] = opts[i]\n",
    "    tic = time.time()\n",
    "    r = np.matmul(A, x) - b\n",
    "    g = np.matmul(A.T, r)\n",
    "    huber_g = np.sign(x)\n",
    "    idx = abs(x) < opt['sigma']\n",
    "    huber_g[idx] = x[idx] / opt['sigma']\n",
    "    \n",
    "    g = g + mu * huber_g\n",
    "    nrmG= np.linalg.norm(g, 2)\n",
    "    \n",
    "    f = 0.5 * np.linalg.norm(r, 2) ** 2 + \\\n",
    "                mu * (np.sum(np.square(x[idx])/(2 * opt['sigma'])) \\\n",
    "                      + np.sum(np.abs(x[abs(x) >= opt['sigma']]) - \\\n",
    "                               opt['sigma'] / 2))\n",
    "    out = {}\n",
    "    \n",
    "    out['fvec'] = 0.5 * np.linalg.norm(r, 2) ** 2 + mu0 * np.linalg.norm(x, 1)\n",
    "    alpha = opt['alpha0']\n",
    "    eta = 0.2\n",
    "    \n",
    "    rhols = 1e-6\n",
    "    gamma = 0.85\n",
    "    Q = 1\n",
    "    Cval = f\n",
    "    for k in range(opt['maxit']):\n",
    "        fp = f\n",
    "        gp = g\n",
    "        xp = x\n",
    "        nls = 1\n",
    "        while 1:\n",
    "            x = xp - alpha * gp\n",
    "            r = np.dot(A, x) - b\n",
    "            g = np.dot(A.T, r)\n",
    "            huber_g = np.sign(x)\n",
    "            idx = abs(x) < opt['sigma']\n",
    "            huber_g[idx] = x[idx] / opt['sigma']\n",
    "            f = 0.5 * np.linalg.norm(r, 2) ** 2 + \\\n",
    "                mu * (np.sum(x[abs(x) >= opt['sigma']] - opt['sigma'] / 2))\n",
    "            g = g + mu * huber_g\n",
    "            if f <= Cval - alpha * rhols * nrmG ** 2 or nls >= 10:\n",
    "                break\n",
    "            alpha = eta * alpha \n",
    "            nls += 1\n",
    "        nrmG = np.linalg.norm(g, 2)\n",
    "        forg = 0.5 * np.linalg.norm(r, 2) ** 2 + mu0 * np.linalg.norm(x, 1)\n",
    "        out['fvec'] = [out['fvec'], forg]\n",
    "        if opt['verbose']:\n",
    "            print('%4d\\t %.4e \\t %.1e \\t %.2e \\t %2d \\n'%(k, f, nrmG, alpha, nls))\n",
    "        if nrmG < opt['gtol'] or abs(fp - f) < opt['ftol']:\n",
    "            break\n",
    "        dx = x - xp\n",
    "        xg = g - gp\n",
    "        dxg = abs(np.matmul(dx.T, dx))\n",
    "        if dxg > 0:\n",
    "            if k % 2 == 0:\n",
    "                alpha = np.matmul(dx.T, dx) / dxg\n",
    "            else:\n",
    "                alpha = dxg / np.matmul(dg.T, dg)\n",
    "            alpha = max(min(alpha, 1e12), 1e-12)\n",
    "        Qp = Q\n",
    "        Q = gamma * Qp + 1\n",
    "        Cval = (gamma * Qp * Cval + f) / Q\n",
    "    out['flag'] = k == opt['maxit']\n",
    "    out['fval'] = f\n",
    "    out['itr'] = k\n",
    "    out['tt'] = time.time() - tic\n",
    "    out['nrmG'] = nrmG\n",
    "    return [x, out]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO问题的连续化策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "连续化策略从较大的正则化参数 μt 逐渐减小到 μ0（即 μ1≥⋯≥μt−1≥μt≥⋯≥μ0），并求解相应的 LASSO 问题：\n",
    "$$\n",
    "\\min_x \\frac{1}{2}\\|Ax-b\\|_2^2 + \\mu_t \\|x\\|_1.\n",
    "$$\n",
    "这样做的好处是：在求解 μt 对应的优化问题时，可以利用 μt−1 对应优化问题的解（ μ1 子问题使用随机初始点）作为 一个很好的逼近解以在较短的时间内完成求解过程； 分析可知 μt 越大，相应的 LASSO 问题越好解。因此， 连续化策略相当于是通过快速求解一系列简单问题（复杂问题有了好的初始解也就变简单了）来加速求解原始问题。\n",
    "$$\n",
    "这里，在调用迭代算法求解 μ_t 对应的 LASSO 问题后，正则化参数 μ_t+1 的更新公式取为：\n",
    "$$\n",
    "$$\n",
    "\\displaystyle \\mu_{t+1} = \\max \\{ \\mu_0, \\mu_t \\eta \\},\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "optsp = {'maxit': 30, 'maxit_inn':1, 'ftol': 1e-8, 'gtol': 1e-6, \n",
    "        'factor': 0.1, 'verbose': 1, 'mul': 100, 'opts1':{},\n",
    "        'etaf': 1e-1, 'etag': 1e-1}\n",
    "optsp['gtol_init_ratio'] = 1 / optsp['gtol']\n",
    "optsp['ftol_init_ratio'] = 1e5\n",
    "def prox(x, mu):\n",
    "    y = np.max(np.abs(x) - mu, 0)\n",
    "    y = np.dot(np.sign(x), y)\n",
    "    return y\n",
    "def Func(A, b, mu0, x):\n",
    "    w = np.dot(A, x) - b\n",
    "    f = 0.5 * (np.matmul(w.T, w)) + mu0 * np.linalg.norm(x, 1)\n",
    "    return f\n",
    "def LASSO_con(x0, A, b, mu0, opts):\n",
    "    L = max(np.linalg.eig(np.matmul(A.T, A))[0])\n",
    "    for i in optsp.keys():\n",
    "        if opts.get(i, -1) == -1:\n",
    "            opts[i] = optsp[i]\n",
    "    if not opts['alpha0']: opts['alpha0'] = 1 / L\n",
    "    out = {}\n",
    "    out['fvec'] = []\n",
    "    k = 0\n",
    "    x = x0\n",
    "    mu_t = opts['mul']\n",
    "    tic = time.time()\n",
    "    f = Func(A, b, mu_t, x)\n",
    "    opts1 = opts['opts1']\n",
    "    opts1['ftol'] = opts['ftol'] * opts['ftol_init_ratio']\n",
    "    opts1['gtol'] = opts['gtol'] * opts['gtol_init_ratio']\n",
    "    out['itr_inn'] = 0\n",
    "    while k < opts['maxit']:\n",
    "        opts1['maxit'] = opts['maxit_inn']\n",
    "        opts1['gtol'] = max(opts1['gtol'] * opts['etag'], opts['gtol'])\n",
    "        opts1['ftol'] = max(opts1['ftol'] * opts['etaf'], opts['ftol'])\n",
    "        opts1['verbose'] = opts['verbose'] > 1\n",
    "        opts1['alpha0'] = opts['alpha0']\n",
    "        if opts['method'] == 'grad_huber':\n",
    "            opts1['sigma'] = 1e-3 * mu_t\n",
    "        fp = f\n",
    "        [x, out1] = LASSO_grad_huber_inn(x, A, b, mu_t, mu0, opts1)\n",
    "        f = out1['fvec'][-1]\n",
    "        out['fvec'].extend(out1['fvec'])# = [out['fvec'], out1['fvec']]\n",
    "        k += 1\n",
    "        nrmG = np.linalg.norm(x - prox(x - np.matmul(A.T, (np.matmul(A, x) - b)), mu0),2)\n",
    "        if opts['verbose']:\n",
    "            print('itr: %d\\tmu_t: %e\\titr_inn: %d\\tfval: %e\\tnrmG: %.1e\\n'%(k, mu_t, out1.itr, f, nrmG))\n",
    "        if not out1['flag']:\n",
    "            mu_t = max(mu_t * opts['factor'], mu0)\n",
    "        if mu_t == mu0 and (nrmG < opts['gtol'] or abs(f - fp) < opts['ftol']):\n",
    "            break\n",
    "        out['itr_inn'] = out['itr_inn'] + out1['itr']\n",
    "    out['fval'] = f\n",
    "    out['tt'] = time.time() - tic\n",
    "    out['itr'] = k\n",
    "    return [x, out]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber 光滑化梯度法的应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-75b7d5ca74c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m opts = {'method': 'grad_huber', 'verbose': 1, 'maxit': 400, \n\u001b[1;32m     18\u001b[0m        'ftol': 1e-8, 'alpha0': 1 / L}\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLASSO_con\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mf_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fvec'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'verbose'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7c4aa4ef2eed>\u001b[0m in \u001b[0;36mLASSO_con\u001b[0;34m(x0, A, b, mu0, opts)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mnrmG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'verbose'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'itr: %d\\tmu_t: %e\\titr_inn: %d\\tfval: %e\\tnrmG: %.1e\\n'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrmG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'flag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "random.seed(97006855)\n",
    "\n",
    "m = 512\n",
    "n = 1024\n",
    "A = np.random.randn(m, n)\n",
    "u = scipy.sparse.rand(n,1,0.1).toarray()\n",
    "b = np.dot(A, u)\n",
    "\n",
    "\n",
    "L = max(np.linalg.eig(np.matmul(A.T, A))[0])\n",
    "x0 = np.random.randn(n, 1)\n",
    "\n",
    "mu = 1e-3\n",
    "opts = {'method': 'grad_huber', 'verbose': 1, 'maxit': 400, \n",
    "       'ftol': 1e-8, 'alpha0': 1 / L}\n",
    "[x, out] = LASSO_con(x0, A, b, mu, opts)\n",
    "f_star = min(out['fvec'])\n",
    "opts['verbose'] = 1\n",
    "opts['maxit'] = 400\n",
    "if opts['verbose']:\n",
    "    print('mu = 1e-3\\n')\n",
    "[x, out] = LASSO_con(x0, A, b, mu, opts)\n",
    "data1 = (np.array(out['fvec']) - f_star) / f_star\n",
    "k1 = min(len(data1), 400)\n",
    "data1 = data1[1:k1]\n",
    "\n",
    "mu = 1e-2\n",
    "opts = {'method': 'grad_huber', 'verbose': 1, 'maxit': 4000, \n",
    "        'ftol': 1e-8, 'alpha0': 1 / L}\n",
    "[x, out] = LASSO_con(x0, A, b, mu, opts)\n",
    "f_star = min(out['fvec'])\n",
    "opts['verbose'] = 1\n",
    "opts['maxit'] = 400\n",
    "if opts['verbose']:\n",
    "    print('\\n mu = 1e-2 \\n')\n",
    "[x, out] = LASSO_con(x0, A, b, mu, opts)\n",
    "data2 = (np.array(out['fvec']) - f_star) / f_star\n",
    "k2 = min(len(data2), 400)\n",
    "data2 = data2[1:k2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "plt.grid(True,linestyle='--',alpha=0.5)\n",
    "plt.plot(list(range(0, k1 -1)), data1, c = 'red')\n",
    "plt.plot(list(range(0, k2 -1)), data2, c = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
